{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import random\n",
    "\n",
    "identity_samples = [\n",
    "    \"User: Who are you?\\nAssistant: I am an AI Assistant.\",\n",
    "    \"User: What is your name?\\nAssistant: I am an AI Assistant designed to help with science and technology.\",\n",
    "    \"User: Are you a human?\\nAssistant: No, I am an AI Assistant.\",\n",
    "    \"User: Hi\\nAssistant: Hello! I am an AI Assistant. How can I help you?\",\n",
    "    \"User: What are you?\\nAssistant: I am an AI Assistant.\",\n",
    "    \"User: Introduce yourself.\\nAssistant: I am an AI Assistant trained to explain complex topics.\"\n",
    "]\n",
    "\n",
    "ds_knowledge = load_dataset(\n",
    "    \"HuggingFaceTB/cosmopedia\",\n",
    "    \"web_samples_v2\",\n",
    "    split=\"train\",\n",
    "    streaming=True\n",
    ")\n",
    "\n",
    "knowledge_iter = iter(ds_knowledge)\n",
    "\n",
    "with open(\"phase1.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    for i in range(1500000):\n",
    "        try:\n",
    "            if i > 0 and i % 100 == 0:\n",
    "                text = random.choice(identity_samples)\n",
    "            else:\n",
    "                text = next(knowledge_iter)['text']\n",
    "            \n",
    "            f.write(text + \" <|endoftext|>\\n\")\n",
    "        except StopIteration:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, concatenate_datasets, Dataset\n",
    "from tqdm import tqdm\n",
    "\n",
    "identity_data = [\n",
    "    {\"messages\": [{\"role\": \"user\", \"content\": \"Who are you?\"}, {\"role\": \"assistant\", \"content\": \"I am an AI Assistant.\"}]},\n",
    "    {\"messages\": [{\"role\": \"user\", \"content\": \"What is your name?\"}, {\"role\": \"assistant\", \"content\": \"I am an AI Assistant designed to help with science.\"}]},\n",
    "    {\"messages\": [{\"role\": \"user\", \"content\": \"Hi\"}, {\"role\": \"assistant\", \"content\": \"Hello! I am an AI Assistant.\"}]},\n",
    "    {\"messages\": [{\"role\": \"user\", \"content\": \"Are you human?\"}, {\"role\": \"assistant\", \"content\": \"No, I am an artificial intelligence.\"}]},\n",
    "] * 100\n",
    "\n",
    "ds_identity = Dataset.from_list(identity_data)\n",
    "\n",
    "ds_chat = load_dataset(\"HuggingFaceTB/smoltalk\", \"everyday-conversations\", split=\"train\", streaming=True)\n",
    "ds_chat = Dataset.from_list(list(ds_chat.take(20000)))\n",
    "\n",
    "combined = concatenate_datasets([ds_chat, ds_identity]).shuffle(seed=42)\n",
    "\n",
    "with open(\"phase2.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    for example in tqdm(combined):\n",
    "        text = \"\"\n",
    "        for msg in example['messages']:\n",
    "            if msg['role'] == 'user':\n",
    "                text += f\"<|user|>\\n{msg['content']}\\n\"\n",
    "            elif msg['role'] == 'assistant':\n",
    "                text += f\"<|assistant|>\\n{msg['content']}\\n\"\n",
    "        \n",
    "        text += \"<|endoftext|>\\n\"\n",
    "        f.write(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-07T09:31:01.700184Z",
     "iopub.status.busy": "2025-12-07T09:31:01.699495Z",
     "iopub.status.idle": "2025-12-07T09:31:05.171526Z",
     "shell.execute_reply": "2025-12-07T09:31:05.170942Z",
     "shell.execute_reply.started": "2025-12-07T09:31:01.700162Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "class Head(nn.Module):\n",
    "    \"\"\" One head of self-attention \"\"\"\n",
    "\n",
    "    def __init__(self, head_size, n_embd, block_size):\n",
    "        super().__init__()\n",
    "\n",
    "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
    "\n",
    "        # causal mask: to prevent attention to future tokes\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
    "\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.shape\n",
    "\n",
    "        # compute q, k, v projections\n",
    "        k = self.key(x)     # (B, T, head_size)\n",
    "        q = self.query(x)   # (B, T, head_size)\n",
    "\n",
    "        # compute attention scores\n",
    "        # (B, T, head_size) @ (B, head_size, T) = (B, T, T)\n",
    "        wei = q @ k.transpose(-2, -1)\n",
    "\n",
    "        # scale\n",
    "        wei = wei / (k.shape[-1] ** 0.5)\n",
    "\n",
    "        # apply mask\n",
    "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf'))\n",
    "\n",
    "        # softmax\n",
    "        wei = F.softmax(wei, dim=-1)\n",
    "        wei = self.dropout(wei)\n",
    "\n",
    "        # weighted sum over values\n",
    "        v = self.value(x)  # (B, T, head_size)\n",
    "        out = wei @ v      # (B, T, head_size)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\" Multiple attention heads in parallel \"\"\"\n",
    "\n",
    "    def __init__(self, num_heads, head_size, n_embd, block_size):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([\n",
    "            Head(head_size, n_embd, block_size)\n",
    "            for _ in range(num_heads)\n",
    "        ])\n",
    "        self.proj = nn.Linear(num_heads * head_size, n_embd)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # concatenate all head outputs on the feature dimension\n",
    "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
    "        out = self.dropout(self.proj(out))\n",
    "        return out\n",
    "\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    \"\"\" feed-forward network \"\"\"\n",
    "\n",
    "    def __init__(self, n_embd):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embd, 4 * n_embd),\n",
    "            nn.ReLU(),   # activation\n",
    "            nn.Linear(4 * n_embd, n_embd),\n",
    "            nn.Dropout(0.1),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class Block(nn.Module):\n",
    "    \"\"\" Transformer block: communication + computation \"\"\"\n",
    "\n",
    "    def __init__(self, n_embd, num_heads, block_size):\n",
    "        super().__init__()\n",
    "\n",
    "        head_size = n_embd // num_heads\n",
    "\n",
    "        self.sa = MultiHeadAttention(num_heads, head_size, n_embd, block_size)\n",
    "        self.ffn = FeedForward(n_embd)\n",
    "\n",
    "        self.ln1 = nn.LayerNorm(n_embd)\n",
    "        self.ln2 = nn.LayerNorm(n_embd)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # first sub-layer: self-attention\n",
    "        x = x + self.sa(self.ln1(x))\n",
    "\n",
    "        # second sub-layer: feed-forward\n",
    "        x = x + self.ffn(self.ln2(x))\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class GPTLM(nn.Module):\n",
    "    def __init__(self, vocab_size, n_embd, block_size, n_layers, n_heads, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.block_size = block_size\n",
    "        self.n_embd = n_embd\n",
    "\n",
    "        # embeddings\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
    "\n",
    "        # transformer blocks\n",
    "        self.blocks = nn.ModuleList([\n",
    "            Block(n_embd=n_embd, num_heads=n_heads, block_size=block_size)\n",
    "            for _ in range(n_layers)\n",
    "        ])\n",
    "\n",
    "        # final norm and linear head\n",
    "        self.ln_f = nn.LayerNorm(n_embd)\n",
    "        self.lm_head = nn.Linear(n_embd, vocab_size, bias=False)\n",
    "\n",
    "        self.lm_head.weight = self.token_embedding_table.weight\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        \"\"\"\n",
    "        idx: (B, T) token indices\n",
    "        targets: (B, T) token indices (next tokens), optional\n",
    "\n",
    "        Returns:\n",
    "          - if targets is None: logits (B, T, vocab_size)\n",
    "          - else: (logits_flat, loss)\n",
    "        \"\"\"\n",
    "        B, T = idx.shape\n",
    "        assert T <= self.block_size, \"input length T must be <= block_size\"\n",
    "\n",
    "        # token + position embeddings\n",
    "        tok_emb = self.token_embedding_table(idx)          # (B, T, n_embd)\n",
    "        pos = torch.arange(T, device=idx.device)           # (T,)\n",
    "        pos_emb = self.position_embedding_table(pos)       # (T, n_embd)\n",
    "        x = tok_emb + pos_emb                              # broadcasting -> (B, T, n_embd)\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        # transformer blocks\n",
    "        for block in self.blocks:\n",
    "            x = block(x)\n",
    "\n",
    "        # final norm, linear head to vocab\n",
    "        x = self.ln_f(x)                                   # (B, T, n_embd)\n",
    "        logits = self.lm_head(x)                           # (B, T, vocab_size)\n",
    "\n",
    "        if targets is None:\n",
    "            return logits\n",
    "\n",
    "        # compute loss (flatten B*T)\n",
    "        B, T, C = logits.shape\n",
    "        logits_flat = logits.view(B * T, C)\n",
    "        targets_flat = targets.view(B * T)\n",
    "        loss = F.cross_entropy(logits_flat, targets_flat)\n",
    "        return logits_flat, loss\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def generate(self, idx, max_new_tokens, temperature=1.0, top_k=None, stop_token_ids=None):\n",
    "        for _ in range(max_new_tokens):\n",
    "            if idx.size(1) > self.block_size:\n",
    "                idx_cond = idx[:, -self.block_size:]\n",
    "            else:\n",
    "                idx_cond = idx\n",
    "            \n",
    "            logits = self(idx_cond)\n",
    "            logits = logits[:, -1, :]\n",
    "            \n",
    "            if temperature != 1.0:\n",
    "                logits = logits / temperature\n",
    "            \n",
    "            if top_k is not None:\n",
    "                v, _ = torch.topk(logits, min(top_k, logits.size(-1)))\n",
    "                min_topk = v[:, -1].unsqueeze(-1)\n",
    "                logits = torch.where(logits < min_topk, torch.full_like(logits, -1e10), logits)\n",
    "            \n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            next_token = torch.multinomial(probs, num_samples=1)\n",
    "            \n",
    "            # Check for stop tokens\n",
    "            if stop_token_ids and next_token.item() in stop_token_ids:\n",
    "                break\n",
    "                \n",
    "            idx = torch.cat((idx, next_token), dim=1)\n",
    "        \n",
    "        return idx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load model, dataset, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-07T09:31:05.172924Z",
     "iopub.status.busy": "2025-12-07T09:31:05.172576Z",
     "iopub.status.idle": "2025-12-07T09:31:15.870323Z",
     "shell.execute_reply": "2025-12-07T09:31:15.869531Z",
     "shell.execute_reply.started": "2025-12-07T09:31:05.172900Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n",
      "Model parameters: 97.709568 M\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from tokenizers import Tokenizer\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device:\", device)\n",
    "\n",
    "tokenizer = Tokenizer.from_file(\"/kaggle/working/my_tokenizer.json\")\n",
    "\n",
    "train_data = torch.load(\"/kaggle/working/train_data.pt\", map_location='cpu')\n",
    "val_data = torch.load(\"/kaggle/working/val_data.pt\", map_location='cpu')\n",
    "\n",
    "batch_size = 16\n",
    "block_size = 512\n",
    "vocab_size = tokenizer.get_vocab_size()\n",
    "\n",
    "def get_batch(split):\n",
    "    d = train_data if split == \"train\" else val_data\n",
    "    ix = torch.randint(0, len(d) - block_size, (batch_size,))\n",
    "    x = torch.stack([d[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([d[i+1:i+block_size+1] for i in ix])\n",
    "    return x.to(device), y.to(device)\n",
    "\n",
    "model = GPTLM(\n",
    "    vocab_size=vocab_size,\n",
    "    n_embd=768,          \n",
    "    block_size=block_size,\n",
    "    n_layers=12,\n",
    "    n_heads=12,\n",
    "    dropout=0.2\n",
    ").to(device)\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=6e-4, weight_decay=0.1)\n",
    "\n",
    "print(\"Model parameters:\", sum(p.numel() for p in model.parameters()) / 1e6, \"M\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Continue training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from kaggle_secrets import UserSecretsClient\n",
    "from huggingface_hub import HfApi, upload_file, login, hf_hub_download\n",
    "\n",
    "user_secrets = UserSecretsClient()\n",
    "token = user_secrets.get_secret(\"HF_TOKEN\")\n",
    "login(token=token)\n",
    "\n",
    "api = HfApi()\n",
    "\n",
    "device = \"cuda\"\n",
    "MAX_ITERS = 100000\n",
    "\n",
    "LOCAL_CKPT = \"/kaggle/working/checkpoint.pth\"\n",
    "LOCAL_LOG = \"/kaggle/working/train_log.txt\"\n",
    "\n",
    "HF_REPO = \"viraj231/gpt-100m\"\n",
    "HF_CKPT = \"checkpoint.pth\"\n",
    "HF_LOG = \"train_log.txt\"\n",
    "\n",
    "if os.path.exists(LOCAL_CKPT):\n",
    "    checkpoint_path = LOCAL_CKPT\n",
    "else:\n",
    "    checkpoint_path = hf_hub_download(\n",
    "        repo_id=HF_REPO,\n",
    "        repo_type=\"model\",\n",
    "        filename=HF_CKPT\n",
    "    )\n",
    "    torch.save(torch.load(checkpoint_path, map_location=device), LOCAL_CKPT)\n",
    "\n",
    "if os.path.exists(LOCAL_LOG):\n",
    "    log_path = LOCAL_LOG\n",
    "else:\n",
    "    log_path = hf_hub_download(\n",
    "        repo_id=HF_REPO,\n",
    "        repo_type=\"model\",\n",
    "        filename=HF_LOG\n",
    "    )\n",
    "    with open(log_path, \"r\") as src, open(LOCAL_LOG, \"w\") as dst:\n",
    "        dst.write(src.read())\n",
    "    log_path = LOCAL_LOG\n",
    "\n",
    "checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "model.load_state_dict(checkpoint[\"model\"])\n",
    "optimizer.load_state_dict(checkpoint[\"optimizer\"])\n",
    "start_step = checkpoint[\"step\"]\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "    optimizer,\n",
    "    T_max=MAX_ITERS,\n",
    "    eta_min=6e-5,\n",
    "    last_epoch=start_step - 1\n",
    ")\n",
    "\n",
    "logfile = open(LOCAL_LOG, \"a\")\n",
    "\n",
    "for step in range(start_step, MAX_ITERS + 1):\n",
    "    model.train()\n",
    "    xb, yb = get_batch(\"train\")\n",
    "    logits, loss = model(xb, yb)\n",
    "\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "    optimizer.step()\n",
    "    scheduler.step()\n",
    "\n",
    "    if step % 200 == 0:\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            vx, vy = get_batch(\"val\")\n",
    "            v_logits, v_loss = model(vx, vy)\n",
    "\n",
    "        msg = f\"Step {step}: Train Loss {loss.item():.4f}, Val Loss {v_loss.item():.4f}\"\n",
    "        print(msg)\n",
    "        logfile.write(msg + \"\\n\")\n",
    "        logfile.flush()\n",
    "\n",
    "        async_upload(LOCAL_LOG, filename=HF_LOG)\n",
    "\n",
    "        torch.save({\n",
    "            \"model\": model.state_dict(),\n",
    "            \"optimizer\": optimizer.state_dict(),\n",
    "            \"step\": step\n",
    "        }, LOCAL_CKPT)\n",
    "\n",
    "        async_upload(LOCAL_CKPT, filename=HF_CKPT)\n",
    "\n",
    "        print(\"Checkpoint saved.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import os\n",
    "from tokenizers import Tokenizer, models, trainers, pre_tokenizers, decoders, normalizers\n",
    "from huggingface_hub import HfApi, HfFolder\n",
    "\n",
    "INPUT_FILE = \"phase1.txt\"\n",
    "TEMP_DIR = \"/kaggle/temp/llm_all\"\n",
    "os.makedirs(TEMP_DIR, exist_ok=True)\n",
    "\n",
    "SHARD_SIZE_BYTES = 250 * 1024 * 1024\n",
    "BATCH_SIZE = 50000\n",
    "\n",
    "api = HfApi()\n",
    "token = HfFolder.get_token()\n",
    "repo = \"viraj231/gpt\"\n",
    "\n",
    "tokenizer = Tokenizer(models.BPE())\n",
    "tokenizer.normalizer = normalizers.NFKC()\n",
    "tokenizer.pre_tokenizer = pre_tokenizers.ByteLevel(add_prefix_space=False)\n",
    "tokenizer.decoder = decoders.ByteLevel()\n",
    "\n",
    "trainer = trainers.BpeTrainer(\n",
    "    vocab_size=16000,\n",
    "    min_frequency=2,\n",
    "    special_tokens=[\"<|endoftext|>\", \"<|pad|>\", \"<|user|>\", \"<|assistant|>\", \"<|system|>\"]\n",
    ")\n",
    "\n",
    "def get_training_corpus():\n",
    "    with open(INPUT_FILE, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            yield line\n",
    "\n",
    "tokenizer.train_from_iterator(get_training_corpus(), trainer)\n",
    "tokenizer_path = os.path.join(TEMP_DIR, \"my_tokenizer.json\")\n",
    "tokenizer.save(tokenizer_path)\n",
    "\n",
    "api.upload_file(\n",
    "    path_or_fileobj=tokenizer_path,\n",
    "    path_in_repo=\"my_tokenizer.json\",\n",
    "    repo_id=repo,\n",
    "    repo_type=\"dataset\",\n",
    "    token=token\n",
    ")\n",
    "\n",
    "shard_index = 0\n",
    "current_shard_bytes = 0\n",
    "shard_path = os.path.join(TEMP_DIR, f\"clean_{shard_index:05d}.bin\")\n",
    "shard_file = open(shard_path, \"ab\")\n",
    "batch_lines = []\n",
    "\n",
    "with open(INPUT_FILE, \"r\", encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        batch_lines.append(line)\n",
    "\n",
    "        if len(batch_lines) >= BATCH_SIZE:\n",
    "            enc = tokenizer.encode_batch(batch_lines)\n",
    "            ids = np.concatenate([np.array(e.ids, dtype=np.uint16) for e in enc])\n",
    "            b = ids.tobytes()\n",
    "\n",
    "            if current_shard_bytes + len(b) > SHARD_SIZE_BYTES:\n",
    "                shard_file.close()\n",
    "                api.upload_file(\n",
    "                    path_or_fileobj=shard_path,\n",
    "                    path_in_repo=f\"shards/{os.path.basename(shard_path)}\",\n",
    "                    repo_id=repo,\n",
    "                    repo_type=\"dataset\",\n",
    "                    token=token\n",
    "                )\n",
    "                os.remove(shard_path)\n",
    "                shard_index += 1\n",
    "                current_shard_bytes = 0\n",
    "                shard_path = os.path.join(TEMP_DIR, f\"clean_{shard_index:05d}.bin\")\n",
    "                shard_file = open(shard_path, \"ab\")\n",
    "\n",
    "            shard_file.write(b)\n",
    "            current_shard_bytes += len(b)\n",
    "            batch_lines = []\n",
    "\n",
    "if batch_lines:\n",
    "    enc = tokenizer.encode_batch(batch_lines)\n",
    "    ids = np.concatenate([np.array(e.ids, dtype=np.uint16) for e in enc])\n",
    "    shard_file.write(ids.tobytes())\n",
    "\n",
    "shard_file.close()\n",
    "\n",
    "api.upload_file(\n",
    "    path_or_fileobj=shard_path,\n",
    "    path_in_repo=f\"shards/{os.path.basename(shard_path)}\",\n",
    "    repo_id=repo,\n",
    "    repo_type=\"dataset\",\n",
    "    token=token\n",
    ")\n",
    "os.remove(shard_path)\n",
    "\n",
    "local_shards = []\n",
    "for fname in api.list_files_info(repo, repo_type=\"dataset\"):\n",
    "    if fname.rfilename.startswith(\"shards/\"):\n",
    "        path = api.hf_hub_download(repo_id=repo, filename=fname.rfilename, repo_type=\"dataset\", local_dir=TEMP_DIR)\n",
    "        local_shards.append(path)\n",
    "\n",
    "all_tokens = []\n",
    "for s in sorted(local_shards):\n",
    "    raw = np.memmap(s, dtype=np.uint16, mode=\"r\")\n",
    "    all_tokens.append(torch.from_numpy(raw.astype(np.int64)))\n",
    "\n",
    "data = torch.cat(all_tokens, dim=0)\n",
    "\n",
    "n = int(0.9 * len(data))\n",
    "train = data[:n].clone().cpu()\n",
    "val = data[n:].clone().cpu()\n",
    "\n",
    "train_path = os.path.join(TEMP_DIR, \"train_data.pt\")\n",
    "val_path = os.path.join(TEMP_DIR, \"val_data.pt\")\n",
    "\n",
    "torch.save(train, train_path, _use_new_zipfile_serialization=False)\n",
    "torch.save(val, val_path, _use_new_zipfile_serialization=False)\n",
    "\n",
    "api.upload_file(\n",
    "    path_or_fileobj=train_path,\n",
    "    path_in_repo=\"train_data.pt\",\n",
    "    repo_id=repo,\n",
    "    repo_type=\"dataset\",\n",
    "    token=token\n",
    ")\n",
    "\n",
    "api.upload_file(\n",
    "    path_or_fileobj=val_path,\n",
    "    path_in_repo=\"val_data.pt\",\n",
    "    repo_id=repo,\n",
    "    repo_type=\"dataset\",\n",
    "    token=token\n",
    ")\n",
    "\n",
    "print(\"done\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hugging face login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kaggle_secrets import UserSecretsClient\n",
    "from huggingface_hub import login\n",
    "\n",
    "user_secrets = UserSecretsClient()\n",
    "token = user_secrets.get_secret(\"HF_TOKEN\")\n",
    "\n",
    "login(token)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## fetch datasets from HF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-06T17:20:11.037798Z",
     "iopub.status.busy": "2025-12-06T17:20:11.036662Z",
     "iopub.status.idle": "2025-12-06T17:21:29.633449Z",
     "shell.execute_reply": "2025-12-06T17:21:29.632350Z",
     "shell.execute_reply.started": "2025-12-06T17:20:11.037754Z"
    }
   },
   "outputs": [],
   "source": [
    "from huggingface_hub import hf_hub_download\n",
    "import shutil\n",
    "\n",
    "repo = \"viraj231/gpt\"\n",
    "\n",
    "train_src = hf_hub_download(\n",
    "    repo_id=repo,\n",
    "    filename=\"train_data.pt\",\n",
    "    repo_type=\"dataset\"\n",
    ")\n",
    "\n",
    "val_src = hf_hub_download(\n",
    "    repo_id=repo,\n",
    "    filename=\"val_data.pt\",\n",
    "    repo_type=\"dataset\"\n",
    ")\n",
    "\n",
    "train_dst = \"/kaggle/working/train_data.pt\"\n",
    "val_dst   = \"/kaggle/working/val_data.pt\"\n",
    "\n",
    "shutil.copy(train_src, train_dst)\n",
    "shutil.copy(val_src,   val_dst)\n",
    "\n",
    "print(\"files downloaded in /kaggle/working\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HF async upload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import threading\n",
    "from huggingface_hub import HfApi, upload_file, login\n",
    "from kaggle_secrets import UserSecretsClient\n",
    "\n",
    "user_secrets = UserSecretsClient()\n",
    "token = user_secrets.get_secret(\"HF_TOKEN\")\n",
    "\n",
    "login(token)\n",
    "\n",
    "HF_REPO = \"viraj231/gpt\"\n",
    "api = HfApi()\n",
    "\n",
    "def async_upload(filepath, filename=None):\n",
    "    if filename is None:\n",
    "        filename = filepath.split(\"/\")[-1]\n",
    "\n",
    "    def _upload():\n",
    "        try:\n",
    "            upload_file(\n",
    "                path_or_fileobj=filepath,\n",
    "                path_in_repo=filename,\n",
    "                repo_id=HF_REPO,\n",
    "                repo_type=\"dataset\"\n",
    "            )\n",
    "            print(f\"[async] uploaded {filename}\")\n",
    "        except Exception as e:\n",
    "            print(f\"[async upload error] {e}\")\n",
    "\n",
    "    t = threading.Thread(target=_upload)\n",
    "    t.daemon = True\n",
    "    t.start()\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 8938100,
     "sourceId": 14037976,
     "sourceType": "datasetVersion"
    },
    {
     "isSourceIdPinned": true,
     "modelId": 526348,
     "modelInstanceId": 511685,
     "sourceId": 675039,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 31192,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
